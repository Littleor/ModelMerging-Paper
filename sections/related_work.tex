\section{Related Work}
Model merging aims to combine multiple task-specific models into a unified model with diverse capabilities~\citep{wortsman2022model}. This approach has gained significant attention in the large model domain, particularly for large language models~\citep{,wei2025unifying, chen2025bring}, due to its advantages of not requiring access to original training datasets and its cost-effective nature. Existing model merging approaches can be categorized based on whether they require training procedures into two main classes: training-free methods and training-based methods. 

\subsection{Training-Free Model Merging Methods}

Training-free methods operate directly on model parameters without requiring additional optimization procedures. These approaches typically manipulate pre-trained model weights through mathematical operations to achieve effective model combination.

Task Arithmetic~\citep{ilharco2022editing} represents a foundational approach in this category, computing task vectors for each task-specific model by subtracting the base model parameters from fine-tuned parameters. The method then merges multiple task-specific models through predefined scaling coefficients based on task importance. However, this approach often suffers from parameter interference when combining multiple tasks. TIES Merging~\citep{yadav2023ties} addresses the parameter conflict problem inherent in Task Arithmetic by proposing a sophisticated conflict resolution mechanism. The method analyzes parameter changes based on both magnitude and direction, trimming conflicting parameters before merging. Specifically, TIES identifies parameters with conflicting signs across different task vectors and resolves these conflicts through magnitude-based voting schemes. DARE~\citep{yu2024language} introduces a different perspective by focusing on parameter sparsification during the merging process. The method applies drop and rescale operations to task vectors, randomly dropping a fraction of parameters and rescaling the remaining ones to maintain the expected magnitude. This approach reduces parameter interference while preserving essential task-specific knowledge, demonstrating that aggressive sparsification can actually improve merging performance.

\subsection{Training-Based Model Merging Methods}

Training-based methods optimize model combination through gradient descent approaches, enabling more sophisticated adaptation of merging strategies. These methods can be further categorized based on their data requirements.

Sample-dependent methods leverage training data to optimize merging parameters. AdaMerging~\citep{yang2023adamerging} employs small datasets to train task-specific and layer-specific merging coefficients through gradient optimization. The method learns adaptive weights that determine the contribution of each task-specific model at different layers, achieving superior performance through fine-grained control over the merging process. The approach demonstrates that even minimal training data can significantly improve merging effectiveness compared to parameter-space methods.
Sample-independent methods eliminate the dependency on training samples while still employing optimization techniques. WUDI~\citep{cheng2025whoever, wei2025unifying} directly aligns the distribution of each layer through gradient descent, ensuring that the output distribution of each layer in the merged model approximates that of individual task-specific models. The method optimizes merging coefficients by minimizing distributional divergences without requiring labeled samples, focusing purely on maintaining activation patterns across different layers.

Although sample-independent optimization methods eliminate the dependency on training samples, they fundamentally operate in parameter space rather than task space for alignment. Therefore, our work focuses on optimization methods that leverage unlabeled samples to bridge the gap between parameter-level and task-level alignment.